{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "MAX_WORDS = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_Dictionary(file_path):\n",
    "    \n",
    "#     # make dictionary\n",
    "#     text_raw = open(file_path, 'r')\n",
    "#     s = text_raw.read().strip()\n",
    "#     s = s.lower()\n",
    "#     words = re.split('[^a-zA-Z]', s)\n",
    "#     dictionary = Counter(words)\n",
    "    \n",
    "#     # remove empty & single caracters\n",
    "#     list_to_remove = dictionary.keys()\n",
    "#     for item in list_to_remove:\n",
    "#         if len(item) <= 1:\n",
    "#             del dictionary[item]\n",
    "    \n",
    "#     # consider only the MAX_WORDS most common words\n",
    "#     dictionary = dictionary.most_common(MAX_WORDS)\n",
    "#     return dictionary\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "    all_words = []\n",
    "    emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for line in m:\n",
    "                words = line.split()\n",
    "                all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-24-a43c726320d6>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-a43c726320d6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def extract_features(mail_dir):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def extract_features(mail_dir):\n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    train_labels = np.zeros(len(files))\n",
    "    count = 0;\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        wordID = 0\n",
    "                    for i,d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            wordID = i\n",
    "                            features_matrix[docID,wordID] = words.count(word)\n",
    "            train_labels[docID] = 0;\n",
    "            filepathTokens = fil.split('/')\n",
    "            lastToken = filepathTokens[len(filepathTokens) - 1]\n",
    "            if lastToken.startswith(\"spmsg\"):\n",
    "                train_labels[docID] = 1;\n",
    "                count = count + 1\n",
    "            docID = docID + 1\n",
    "    return features_matrix, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAIN_DEM = \"/home/joao/ML/Supervised_Learning/twitter/Dem_laguage.txt\"\n",
    "# TRAIN_REP = \"/home/joao/ML/Supervised_Learning/twitter/Rep_laguage.txt\"\n",
    "\n",
    "TRAIN_DIR = \"../train-tweets\"\n",
    "TEST_DIR = \"../test-tweets\"\n",
    "\n",
    "# dem_dictionary = make_Dictionary(TRAIN_DEM)\n",
    "# rep_dictionary = make_Dictionary(TRAIN_REP)\n",
    "\n",
    "dictionary = make_Dictionary(TRAIN_DIR)\n",
    "\n",
    "features_matrix, labels = extract_features(TRAIN_DIR)\n",
    "\n",
    "print labels\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
